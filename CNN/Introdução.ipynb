{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rede Neural Convolucional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Redes Neurais Convolucionais (também chamadas de ConvNet) alavancam informações e são, portanto, muito adequadas para classificar imagens. Essas redes usam uma arquitetura ad hoc inspirada em dados biológicos retirados de dados fisiológicos esperimentos feitos no córtex visual. Como discutido, nossa visão é baseada em múltiplos níveis de córtex, cada um reconhecendo cada vez mais informação. Primeiro, vemos pixels únicos; então deles, nós reconhecemos simples formas geométricas. E então mais e mais elementos sofisticados, como obejtos, rostos, corpos humanos , animais e assim por diante."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma Rede Neural Convolucional Profunda (DCNN) consiste em muitas camadas de rede. Dois tipos diferentes de camadas, convolucional e pooling , são normalmente alternado. A profundidade de cada filtro aumenta da esquerda paraa a direita na rede. O ultimo estágio é tipicamente feito de uma ou mais camadas totalmente conectadas.\n",
    "\n",
    "Existem três intuições chave além do ConvNet:\n",
    "- Campos receptivos locais\n",
    "- Pesos compartilhados \n",
    "- Pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Campos Receptivos Locais\n",
    "\n",
    "Se queremos preservar a informação espacial, então é conveniente representar cada imagem com uma matriz de pixels. Então, uma maneira simples de codificar a estrutura local é conectar uma submatriz de neurônios de entrada adjacentes em um único oculto\n",
    "neurônio pertencente à próxima camada. Esse único neurônio oculto representa um campo receptivo local. Note que esta operação é chamada de convolução e dá o nome para esse tipo de rede."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pesos e Viés Compartilhados\n",
    "\n",
    "Vamos supor que queremos nos afastar da representação de pixels em uma linha ganhando a capacidade de detectar o mesmo recurso independentemente do local onde é colocado na imagem de entrada. Uma intuição simples é usar o mesmo conjunto de pesos e bias para todos os neurônios nas camadas ocultas. Desta forma, cada camada aprenderá um conjunto de recursos latentes independentes da posição derivada da imagem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Camada Pooling\n",
    "\n",
    "Supondo que queremos resumir a saída de um mapa de recursos. Mais uma vez, nós podemos usar a contiguidade espacial da saída produzida a partir de um único mapa de recursos e agregar os valores de uma submatriz em um único valor de saída que descreve sinteticamente o significado associado a essa região física."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementação com o Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para definir o código LeNet(CNN para detectar o MNIST), usamos o módulo 2D convolucional, que é:\n",
    "\n",
    "keras.layers.convolutional.Conv2D(filters,Kernel_size,padding='valid')\n",
    "\n",
    "- Aqui , filters é o número de kernels de convolução a serem usados (por exemplo,dimensionalidade da saída), o kernel_size é um inteiro ou tupla/lista de dois inteiros, especificando a largura e altura da janela de convolução 2D (pode ser um único inteiro para especificar o mesmo valor para todas as dimensões espaciais), e padding = 'same' significa que o preenchimento é usado.\n",
    "\n",
    "- Existem duas opções de padding: padding ='valid' significa que a convolução é computada apenas onde a entrada e o filtro se sobrepõem completamente e portanto a saída é menor que a entrada. Enquanto padding = 'same' significa que temos uma saída que é do memso tamanho a entrada , para a qual a área ao redor da entrada é preenchida com zeros.\n",
    "\n",
    "Além disso,usamos um módulo MaxPooling2D: \n",
    "\n",
    "keras.layers.pooling.MaxPooling2D(pool_size=(2,2),strides=(2,2))\n",
    "\n",
    "Aqui, pool_size = (2, 2) é uma tupla de dois inteiros representando os fatores por qual a imagem é reduzida verticalmente e horizontalmente. Então (2, 2) irá reduzir pela metade a imagem em cada dimensão e strides = (2, 2) é a passada usada para o processamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as k\n",
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.core import Flatten\n",
    "from keras.layers.core import Dense\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "from keras.optimizers import SGD,RMSprop,Adam\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definindo a ConvNet\n",
    "\n",
    "class LeNet:\n",
    "    @staticmethod\n",
    "    def build(input_shape,classes):\n",
    "        model = Sequential()\n",
    "        #CONV => RELU => POOL\n",
    "        model.add(Conv2D(20,kernel_size=5,padding=\"same\",\n",
    "                                input_shape=input_shape))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
    "        \n",
    "        #CONV => RELU => POOL\n",
    "        model.add(Conv2D(50,kernel_size=5,border_mode=\"same\"))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
    "        \n",
    "        #Flatten => RELu layers\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(500))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        #softmax classifier\n",
    "        model.add(Dense(classes))\n",
    "        model.add(Activation(\"softmax\"))\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "60000 test samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paic-jailson/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:14: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(50, kernel_size=5, padding=\"same\")`\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 88s 2ms/step - loss: 0.1831 - acc: 0.9429 - val_loss: 0.0598 - val_acc: 0.9812\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 87s 2ms/step - loss: 0.0484 - acc: 0.9843 - val_loss: 0.0491 - val_acc: 0.9842\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 87s 2ms/step - loss: 0.0342 - acc: 0.9892 - val_loss: 0.0325 - val_acc: 0.9904\n",
      "Epoch 4/20\n",
      "48000/48000 [==============================] - 88s 2ms/step - loss: 0.0234 - acc: 0.9928 - val_loss: 0.0355 - val_acc: 0.9896\n",
      "Epoch 5/20\n",
      "48000/48000 [==============================] - 87s 2ms/step - loss: 0.0170 - acc: 0.9943 - val_loss: 0.0353 - val_acc: 0.9894\n",
      "Epoch 6/20\n",
      "48000/48000 [==============================] - 87s 2ms/step - loss: 0.0132 - acc: 0.9954 - val_loss: 0.0419 - val_acc: 0.9867\n",
      "Epoch 7/20\n",
      "48000/48000 [==============================] - 88s 2ms/step - loss: 0.0109 - acc: 0.9961 - val_loss: 0.0330 - val_acc: 0.9913\n",
      "Epoch 8/20\n",
      "48000/48000 [==============================] - 87s 2ms/step - loss: 0.0092 - acc: 0.9967 - val_loss: 0.0416 - val_acc: 0.9896\n",
      "Epoch 9/20\n",
      "48000/48000 [==============================] - 87s 2ms/step - loss: 0.0080 - acc: 0.9972 - val_loss: 0.0402 - val_acc: 0.9896\n",
      "Epoch 10/20\n",
      "48000/48000 [==============================] - 88s 2ms/step - loss: 0.0077 - acc: 0.9973 - val_loss: 0.0382 - val_acc: 0.9902\n",
      "Epoch 11/20\n",
      "48000/48000 [==============================] - 88s 2ms/step - loss: 0.0061 - acc: 0.9980 - val_loss: 0.0377 - val_acc: 0.9915\n",
      "Epoch 12/20\n",
      "48000/48000 [==============================] - 88s 2ms/step - loss: 0.0052 - acc: 0.9984 - val_loss: 0.0375 - val_acc: 0.9910\n",
      "Epoch 13/20\n",
      "48000/48000 [==============================] - 87s 2ms/step - loss: 0.0042 - acc: 0.9986 - val_loss: 0.0543 - val_acc: 0.9887\n",
      "Epoch 14/20\n",
      "48000/48000 [==============================] - 88s 2ms/step - loss: 0.0055 - acc: 0.9982 - val_loss: 0.0430 - val_acc: 0.9900\n",
      "Epoch 15/20\n",
      "48000/48000 [==============================] - 88s 2ms/step - loss: 0.0043 - acc: 0.9984 - val_loss: 0.0437 - val_acc: 0.9908\n",
      "Epoch 16/20\n",
      "48000/48000 [==============================] - 87s 2ms/step - loss: 0.0044 - acc: 0.9986 - val_loss: 0.0408 - val_acc: 0.9906\n",
      "Epoch 17/20\n",
      "48000/48000 [==============================] - 88s 2ms/step - loss: 0.0047 - acc: 0.9985 - val_loss: 0.0410 - val_acc: 0.9908\n",
      "Epoch 18/20\n",
      "48000/48000 [==============================] - 88s 2ms/step - loss: 0.0024 - acc: 0.9992 - val_loss: 0.0448 - val_acc: 0.9907\n",
      "Epoch 19/20\n",
      "48000/48000 [==============================] - 88s 2ms/step - loss: 0.0051 - acc: 0.9984 - val_loss: 0.0372 - val_acc: 0.9914\n",
      "Epoch 20/20\n",
      "48000/48000 [==============================] - 88s 2ms/step - loss: 0.0032 - acc: 0.9990 - val_loss: 0.0430 - val_acc: 0.9913\n",
      "10000/10000 [==============================] - 9s 909us/step\n",
      "Test score: 0.038212805645506706\n",
      "Test acc: 0.9915\n"
     ]
    }
   ],
   "source": [
    "#REde e Training\n",
    "NB_EPOCH = 20\n",
    "BATCH_SIZE = 128\n",
    "VERBOSE = 1\n",
    "OPTIMIZER = Adam()\n",
    "VALIDATION_SPLIT = 0.2\n",
    "IMG_ROWS,IMG_COLS = 28,28 #input image dimensions\n",
    "NB_CLASSES = 10 #numero de saídas\n",
    "INPUT_SHAPE = (1,IMG_ROWS,IMG_COLS)\n",
    "\n",
    "#dividindo os dados em treino e teste\n",
    "(X_train,y_train),(X_test,y_test) = mnist.load_data()\n",
    "k.set_image_dim_ordering(\"th\")\n",
    "\n",
    "#normalizando os dados\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "X_train = X_train[:, np.newaxis, :, :]\n",
    "X_test = X_test[:, np.newaxis, :, :]\n",
    "print(X_train.shape[0],'train samples')\n",
    "print(X_train.shape[0],'test samples')\n",
    "\n",
    "#converter vetores de  classe em matrizes de classe binária\n",
    "y_train = np_utils.to_categorical(y_train,NB_CLASSES)\n",
    "y_test = np_utils.to_categorical(y_test,NB_CLASSES)\n",
    "\n",
    "#Inicializador do otimizador do modelo\n",
    "model = LeNet.build(input_shape=INPUT_SHAPE,classes=NB_CLASSES)\n",
    "model.compile(loss=\"categorical_crossentropy\",optimizer=OPTIMIZER,\n",
    "             metrics=[\"accuracy\"])\n",
    "\n",
    "network = model.fit(X_train,y_train,batch_size=BATCH_SIZE,\n",
    "                   epochs=NB_EPOCH,verbose=VERBOSE,validation_split=VALIDATION_SPLIT)\n",
    "\n",
    "score = model.evaluate(X_test,y_test,verbose=VERBOSE)\n",
    "print(\"Test score:\",score[0])\n",
    "print(\"Test acc:\",score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

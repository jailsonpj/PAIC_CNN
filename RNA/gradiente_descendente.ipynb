{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradiente Descendente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É um método de otimização usado na aprendizagem de máquina. Se tratando de uma simples regressão linear, o gradiente descendente só é recomendado quando temos dados com muitas dimensões. Nesse caso, a inversão da matriz X^t X comeca a demorar muito a resolver a regressão linear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problemas convexos têm apenas um mínimo; ou seja, apenas um lugar onde a inclinação é exatamente 0. Esse mínimo é onde a função de perda converge.\n",
    "\n",
    "Calculando a função de perda para cada valor concebível ao longo de todo o conjunto de dados seria uma maneira ineficiente de encontrar o ponto de convergência. Existe uma maneira de examinar esse mecanismo melhor chamado gradiente de descida.\n",
    "\n",
    "O primeiro estágio no gradiente de descida é escolher um valor inicial (um ponto inicial). O ponto de partida não importa muito; Portanto, muitos algoritmos simplesmente definem w1 a 0 ou selecionam um valor aleatório.\n",
    "\n",
    "O algoritmo de gradiente descendente calcula então o gradiente da curva de perda no ponto inicial. Na figura, o gradiente de perda é igual a derivada(inclinação) da curva, e indica qual é caminho mais \"quente\" ou \"mais frio\". Quando há vários pesos, o gradiente é um vetor de derivadas parciais com relação aos pesos.\n",
    "\n",
    "O gradiente sempre aponta na direção do aumento mais acentuado na função de perda. O algoritmo de descida de gradiente dá um passo na direção do gradiente negativo para reduzir a perda o mais rápido possível.\n",
    "\n",
    "\"A descida de gradiente depende de gradientes negativos\"\n",
    "\n",
    "Para detrminar o próximo ponto ao longo da curva da função de perda, o algoritmo de descida de gradiente adiciona alguma fração da magnitude do gradiente ao ponto de partida."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reduzindo Perdas: Taxa de Aprendizagem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como observado, o vetor gradiente tem uma direção e uma magnitude. Os algoritmos de descida de gradiente multiplicam o gradiente por um escalar conhecido como Taxa de aprendizado (tbm chamado de tamanho de etapa) para determinar o próximo ponto. Por exemplo, se a magnitude do gradiente for 2,5 e ataxa de aprendizado for 0,01 , o algoritmo de descida de gradiente escolherá o próximo ponto 0,025 do ponto anterior.\n",
    "\n",
    "Os Hiperparâmetros são os botões que os programadores ajustam nos algoritmos de aprendizado de máquina. A maioria dos programadores de aprendizado de máquina gasta um tempo ajustando a taxa de aprendizado. Se caso a taxa de aprendizado for muito pequena, o aprendizado demorará demais. Por outro lado, se a taxa de aprendizado for muito grande, o próximo ponto irá perpentuamente saltar aleatoriamente pela parte parte inferior do poço, como um experimento de macânica quântica que deu terrivelmente errado.\n",
    "\n",
    "Há uma taxa de aprendizado de Goldilocks[ https://en.wikipedia.org/wiki/Goldilocks_principle ] para cada problema de regressão. O valor de cachinhos Dourados está relacionado a quão plana é a função de perda. Se você sabe que o gradiente da função de perda é pequeno, então você pode tentar com segurança uma taxa de aprendizado maior, que compensa o pequeno gradiente e resulta em um tamanho de etapa maior.\n",
    "\n",
    "1/f(x)^n , taxa de apredizado ideal para 2 ou mais dimensões. É o inverso da Hessiana(matriz da segundas derivadsa parciais)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perda Reduzindo: Descida Estocástica de Gradiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
